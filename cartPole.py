import gymnasium as gym  # Gymnasium ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸° (ê°•í™”í•™ìŠµ í™˜ê²½ ì œê³µ)
import numpy as np  # ìˆ«ì ê³„ì‚°ì„ ìœ„í•œ NumPy
import torch  # PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹ ê²½ë§ êµ¬í˜„
import torch.nn as nn  # ì‹ ê²½ë§ êµ¬ì„± ìš”ì†Œ ë¶ˆëŸ¬ì˜¤ê¸°
import torch.optim as optim  # ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì œê³µ
import random  # ëœë¤ ê°’ ìƒì„±
import matplotlib.pyplot as plt  # í•™ìŠµ ê³¼ì • ì‹œê°í™”ë¥¼ ìœ„í•œ Matplotlib
from collections import deque  # ê²½í—˜ ì¬ìƒ ë©”ëª¨ë¦¬ (Replay Buffer) êµ¬í˜„

# ğŸ’¡ í™˜ê²½ ìƒì„±: 'CartPole-v1'ì€ ë§‰ëŒ€ê¸°ë¥¼ ê· í˜• ìˆê²Œ ìœ ì§€í•˜ëŠ” RL ë¬¸ì œ
env = gym.make("CartPole-v1")
state_dim = env.observation_space.shape[0]  # ìƒíƒœ(State) ê³µê°„ í¬ê¸° (4ì°¨ì›)
action_dim = env.action_space.n  # í–‰ë™(Action) ê³µê°„ í¬ê¸° (2ì°¨ì›: ì™¼ìª½/ì˜¤ë¥¸ìª½)

# ğŸ¯ Q-Network ì •ì˜ (DQNì˜ ì‹ ê²½ë§)
class QNetwork(nn.Module):
    def __init__(self, state_dim, action_dim):
        super(QNetwork, self).__init__()
        self.fc1 = nn.Linear(state_dim, 128)  # ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ (128ê°œ ë‰´ëŸ°)
        self.fc2 = nn.Linear(128, 128)  # ë‘ ë²ˆì§¸ ì€ë‹‰ì¸µ (128ê°œ ë‰´ëŸ°)
        self.fc3 = nn.Linear(128, action_dim)  # ì¶œë ¥ì¸µ (í–‰ë™ì˜ Qê°’ ì¶œë ¥)

    def forward(self, x):
        x = torch.relu(self.fc1(x))  # ì²« ë²ˆì§¸ ì¸µ í™œì„±í™” í•¨ìˆ˜ (ReLU)
        x = torch.relu(self.fc2(x))  # ë‘ ë²ˆì§¸ ì¸µ í™œì„±í™” í•¨ìˆ˜ (ReLU)
        return self.fc3(x)  # í–‰ë™(Action)ì— ëŒ€í•œ Qê°’ ì¶œë ¥

# ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (í•™ìŠµ ì†ë„, íƒìƒ‰ë¥  ë“±)
learning_rate = 0.001  # í•™ìŠµë¥  (Learning Rate)
gamma = 0.99  # í• ì¸ìœ¨ (Discount Factor)
epsilon = 1.0  # ì´ˆê¸° íƒìƒ‰ë¥  (Exploration Rate)
epsilon_decay = 0.995  # íƒìƒ‰ë¥  ê°ì†Œìœ¨ (Epsilon Decay)
epsilon_min = 0.01  # ìµœì†Œ íƒìƒ‰ë¥ 
batch_size = 64  # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
memory_size = 10000  # ê²½í—˜ ì¬ìƒ ë©”ëª¨ë¦¬ í¬ê¸°
target_update = 10  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸ ì£¼ê¸°
episodes = 500  # ì´ í•™ìŠµ ì—í”¼ì†Œë“œ ìˆ˜

# ğŸš€ ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # GPU ì‚¬ìš© ì—¬ë¶€ í™•ì¸
q_network = QNetwork(state_dim, action_dim).to(device)  # í˜„ì¬ í•™ìŠµ ë„¤íŠ¸ì›Œí¬
target_network = QNetwork(state_dim, action_dim).to(device)  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬
target_network.load_state_dict(q_network.state_dict())  # ì´ˆê¸° íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ë™ê¸°í™”
optimizer = optim.Adam(q_network.parameters(), lr=learning_rate)  # Adam ì˜µí‹°ë§ˆì´ì € ì„¤ì •

# ğŸ”„ ê²½í—˜ ì¬ìƒ ë©”ëª¨ë¦¬ (Replay Buffer) ìƒì„±
memory = deque(maxlen=memory_size)  # ìµœëŒ€ í¬ê¸° 10,000ìœ¼ë¡œ ì„¤ì •

# ğŸ­ í–‰ë™ ì„ íƒ í•¨ìˆ˜ (Epsilon-Greedy Policy)
def select_action(state, epsilon):
    if random.random() < epsilon:  # íƒìƒ‰: ëœë¤ í–‰ë™ ì„ íƒ
        return env.action_space.sample()
    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)  # ìƒíƒœ ë³€í™˜
    with torch.no_grad():  # í•™ìŠµ ì—†ì´ Qê°’ ê³„ì‚°
        q_values = q_network(state_tensor)
    return torch.argmax(q_values).item()  # ê°€ì¥ ë†’ì€ Qê°’ì„ ê°€ì§„ í–‰ë™ ì„ íƒ

# ğŸ‹ï¸â€â™‚ï¸ í•™ìŠµ í•¨ìˆ˜ (DQN ì—…ë°ì´íŠ¸)
def train():
    if len(memory) < batch_size:  # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°ë³´ë‹¤ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ë©´ í•™ìŠµ X
        return

    batch = random.sample(memory, batch_size)  # ë©”ëª¨ë¦¬ì—ì„œ ë°°ì¹˜ í¬ê¸°ë§Œí¼ ìƒ˜í”Œë§
    states, actions, rewards, next_states, dones = zip(*batch)  # ë°°ì¹˜ ë°ì´í„° ë¶„ë¦¬

    # Tensor ë³€í™˜ ë° GPU í• ë‹¹
    states = torch.FloatTensor(states).to(device)
    actions = torch.LongTensor(actions).to(device)
    rewards = torch.FloatTensor(rewards).to(device)
    next_states = torch.FloatTensor(next_states).to(device)
    dones = torch.FloatTensor(dones).to(device)

    # ğŸ¯ í˜„ì¬ Qê°’ ê³„ì‚°
    current_q_values = q_network(states).gather(1, actions.unsqueeze(1)).squeeze()
    next_q_values = target_network(next_states).max(1)[0]  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ Qê°’
    target_q_values = rewards + gamma * next_q_values * (1 - dones)  # ì—…ë°ì´íŠ¸ ê³µì‹

    # ì†ì‹¤(Loss) ê³„ì‚° ë° ìµœì í™” ìˆ˜í–‰
    loss = nn.MSELoss()(current_q_values, target_q_values)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# ğŸ“Š í•™ìŠµ ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸
episode_rewards = []

# ğŸ”¥ í•™ìŠµ ì‹¤í–‰
for episode in range(episodes):
    state, _ = env.reset()
    total_reward = 0
    
    for step in range(200):  # í•œ ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ 200 ìŠ¤í…
        action = select_action(state, epsilon)  # í–‰ë™ ì„ íƒ
        next_state, reward, done, _, _ = env.step(action)  # í™˜ê²½ ì§„í–‰
        
        memory.append((state, action, reward, next_state, done))  # ê²½í—˜ ì €ì¥
        state = next_state  # ìƒíƒœ ì—…ë°ì´íŠ¸
        total_reward += reward

        train()  # í•™ìŠµ ìˆ˜í–‰

        if done:
            break  # í™˜ê²½ ì¢…ë£Œ ì‹œ íƒˆì¶œ

    epsilon = max(epsilon * epsilon_decay, epsilon_min)  # íƒìƒ‰ë¥  ê°ì†Œ
    episode_rewards.append(total_reward)  # ë³´ìƒ ì €ì¥

    if episode % target_update == 0:
        target_network.load_state_dict(q_network.state_dict())  # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸

    print(f"Episode {episode}: Total Reward = {total_reward}")

env.close()

# ğŸ“ˆ í•™ìŠµ ê³¼ì • ì‹œê°í™” (Total Reward ê·¸ë˜í”„)
plt.plot(range(episodes), episode_rewards)
plt.xlabel("Episodes")
plt.ylabel("Total Reward")
plt.title("Reinforcement Learning Training Progress")
plt.show()

# ğŸ¥ í•™ìŠµëœ ì—ì´ì „íŠ¸ ì‹¤í–‰ (í™˜ê²½ ì‹œê°í™”)
env = gym.make("CartPole-v1", render_mode="human")  # ì¸ê°„ì´ ë³¼ ìˆ˜ ìˆë„ë¡ ì‹œê°í™” ëª¨ë“œ í™œì„±í™”
for episode in range(5):  # 5ê°œ ì—í”¼ì†Œë“œ ì‹¤í–‰í•˜ì—¬ í–‰ë™ í™•ì¸
    state, _ = env.reset()
    total_reward = 0
    
    for step in range(200):
        env.render()  # í™˜ê²½ í™”ë©´ ì¶œë ¥
        action = select_action(state, 0)  # í•™ìŠµëœ í–‰ë™ ì‚¬ìš© (íƒìƒ‰ ì—†ì´ ê²°ì •ë¡ ì  ì„ íƒ)
        next_state, reward, done, _, _ = env.step(action)
        state = next_state
        total_reward += reward

        if done:
            break  # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ íƒˆì¶œ

    print(f"Test Episode {episode}: Total Reward = {total_reward}")

env.close()
